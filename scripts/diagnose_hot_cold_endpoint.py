#!/usr/bin/env python3
"""
Diagnostic script to measure performance of the hot/cold numbers endpoint.
Identifies bottlenecks in the analytics overview calculation.

Usage:
    python scripts/diagnose_hot_cold_endpoint.py
"""

import time
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger


def measure_time(func_name: str):
    """Decorator to measure function execution time"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start = time.perf_counter()
            result = func(*args, **kwargs)
            elapsed_ms = (time.perf_counter() - start) * 1000
            print(f"  â±ï¸  {func_name}: {elapsed_ms:.2f}ms")
            return result, elapsed_ms
        return wrapper
    return decorator


def diagnose_analytics_endpoint():
    """Run detailed performance diagnosis on the analytics endpoint"""

    print("\n" + "=" * 60)
    print("ðŸ” DIAGNOSTICO DE RENDIMIENTO: /api/v3/analytics/overview")
    print("=" * 60 + "\n")

    timings = {}
    total_start = time.perf_counter()

    # ============================================================
    # STEP 1: Database - Get All Draws
    # ============================================================
    print("ðŸ“Š PASO 1: Carga de datos histÃ³ricos desde SQLite")
    print("-" * 50)

    start = time.perf_counter()
    try:
        from src.database import get_all_draws
        draws_df = get_all_draws()
        elapsed = (time.perf_counter() - start) * 1000
        timings['db_get_all_draws'] = elapsed
        print(f"  â±ï¸  get_all_draws(): {elapsed:.2f}ms")
        print(f"  ðŸ“ˆ Filas cargadas: {len(draws_df)}")
        print(f"  ðŸ“‹ Columnas: {list(draws_df.columns)}")
    except Exception as e:
        print(f"  âŒ ERROR: {e}")
        return

    if draws_df.empty:
        print("  âš ï¸  No hay datos en la base de datos!")
        return

    print()

    # ============================================================
    # STEP 2: Initialize Analysis Components
    # ============================================================
    print("ðŸ§  PASO 2: InicializaciÃ³n de componentes de anÃ¡lisis")
    print("-" * 50)

    start = time.perf_counter()
    from src.v2.statistical_core import (
        TemporalDecayModel,
        MomentumAnalyzer,
        GapAnalyzer,
        PatternEngine
    )
    elapsed = (time.perf_counter() - start) * 1000
    timings['import_modules'] = elapsed
    print(f"  â±ï¸  Import modules: {elapsed:.2f}ms")

    start = time.perf_counter()
    temporal_model = TemporalDecayModel(decay_factor=0.05)
    elapsed = (time.perf_counter() - start) * 1000
    timings['init_temporal_model'] = elapsed
    print(f"  â±ï¸  TemporalDecayModel.__init__(): {elapsed:.2f}ms")

    start = time.perf_counter()
    momentum_analyzer = MomentumAnalyzer(short_window=10, long_window=50)
    elapsed = (time.perf_counter() - start) * 1000
    timings['init_momentum'] = elapsed
    print(f"  â±ï¸  MomentumAnalyzer.__init__(): {elapsed:.2f}ms")

    start = time.perf_counter()
    gap_analyzer = GapAnalyzer()
    elapsed = (time.perf_counter() - start) * 1000
    timings['init_gap'] = elapsed
    print(f"  â±ï¸  GapAnalyzer.__init__(): {elapsed:.2f}ms")

    start = time.perf_counter()
    pattern_engine = PatternEngine()
    elapsed = (time.perf_counter() - start) * 1000
    timings['init_pattern'] = elapsed
    print(f"  â±ï¸  PatternEngine.__init__(): {elapsed:.2f}ms")

    print()

    # ============================================================
    # STEP 3: Perform Analysis (THE HEAVY LIFTING)
    # ============================================================
    print("ðŸ”¬ PASO 3: EjecuciÃ³n de anÃ¡lisis (CÃLCULOS PESADOS)")
    print("-" * 50)

    start = time.perf_counter()
    weights = temporal_model.calculate_weights(draws_df)
    elapsed = (time.perf_counter() - start) * 1000
    timings['temporal_weights'] = elapsed
    print(f"  â±ï¸  temporal_model.calculate_weights(): {elapsed:.2f}ms")
    print(f"      â†’ Window size: {weights.window_size} draws")

    start = time.perf_counter()
    momentum = momentum_analyzer.analyze(draws_df)
    elapsed = (time.perf_counter() - start) * 1000
    timings['momentum_analysis'] = elapsed
    print(f"  â±ï¸  momentum_analyzer.analyze(): {elapsed:.2f}ms")

    start = time.perf_counter()
    gaps = gap_analyzer.analyze(draws_df)
    elapsed = (time.perf_counter() - start) * 1000
    timings['gap_analysis'] = elapsed
    print(f"  â±ï¸  gap_analyzer.analyze(): {elapsed:.2f}ms")

    start = time.perf_counter()
    patterns = pattern_engine.analyze(draws_df)
    elapsed = (time.perf_counter() - start) * 1000
    timings['pattern_analysis'] = elapsed
    print(f"  â±ï¸  pattern_engine.analyze(): {elapsed:.2f}ms")

    print()

    # ============================================================
    # STEP 4: Build Response Objects
    # ============================================================
    print("ðŸ“¦ PASO 4: ConstrucciÃ³n de objetos de respuesta")
    print("-" * 50)

    import numpy as np
    from src.v2.analytics_api import (
        HotColdNumbers,
        MomentumReport,
        GapReport,
        PatternStats
    )

    start = time.perf_counter()
    # Hot/cold analysis
    wb_indices = np.argsort(weights.white_ball_weights)
    pb_indices = np.argsort(weights.powerball_weights)
    hot_wb = [int(i + 1) for i in wb_indices[-10:][::-1]]
    cold_wb = [int(i + 1) for i in wb_indices[:10]]
    hot_pb = [int(i + 1) for i in pb_indices[-5:][::-1]]
    cold_pb = [int(i + 1) for i in pb_indices[:5]]
    elapsed = (time.perf_counter() - start) * 1000
    timings['build_hot_cold'] = elapsed
    print(f"  â±ï¸  Build hot/cold analysis: {elapsed:.2f}ms")
    print(f"      â†’ Hot white balls: {hot_wb[:5]}")
    print(f"      â†’ Cold white balls: {cold_wb[:5]}")

    print()

    # ============================================================
    # STEP 5: Co-occurrences from Database
    # ============================================================
    print("ðŸ”— PASO 5: Obtener co-ocurrencias desde DB")
    print("-" * 50)

    start = time.perf_counter()
    from src.database import get_db_connection
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT number_a, number_b, count, deviation_pct
            FROM cooccurrences
            WHERE is_significant = TRUE
            ORDER BY deviation_pct DESC
            LIMIT 10
        """)
        cooccurrences = cursor.fetchall()
        conn.close()
        elapsed = (time.perf_counter() - start) * 1000
        timings['get_cooccurrences'] = elapsed
        print(f"  â±ï¸  Query cooccurrences: {elapsed:.2f}ms")
        print(f"      â†’ Pairs found: {len(cooccurrences)}")
    except Exception as e:
        elapsed = (time.perf_counter() - start) * 1000
        timings['get_cooccurrences'] = elapsed
        print(f"  âš ï¸  Cooccurrences query failed: {e} ({elapsed:.2f}ms)")

    print()

    # ============================================================
    # SUMMARY
    # ============================================================
    total_elapsed = (time.perf_counter() - total_start) * 1000

    print("=" * 60)
    print("ðŸ“Š RESUMEN DE RENDIMIENTO")
    print("=" * 60)

    # Sort by time descending
    sorted_timings = sorted(timings.items(), key=lambda x: x[1], reverse=True)

    print("\nðŸ”´ TOP CUELLOS DE BOTELLA (ordenado por tiempo):")
    print("-" * 50)
    for name, elapsed in sorted_timings:
        pct = (elapsed / total_elapsed) * 100
        bar_len = int(pct / 2)
        bar = "â–ˆ" * bar_len
        status = "ðŸ”´" if elapsed > 100 else "ðŸŸ¡" if elapsed > 50 else "ðŸŸ¢"
        print(f"  {status} {name:30s} {elapsed:8.2f}ms ({pct:5.1f}%) {bar}")

    print("\n" + "-" * 50)
    print(f"â±ï¸  TIEMPO TOTAL: {total_elapsed:.2f}ms")

    if total_elapsed > 500:
        print("ðŸ”´ LENTO: MÃ¡s de 500ms - Necesita optimizaciÃ³n")
    elif total_elapsed > 200:
        print("ðŸŸ¡ MODERADO: Entre 200-500ms - Considerar cache")
    else:
        print("ðŸŸ¢ RÃPIDO: Menos de 200ms - Buen rendimiento")

    print("\n" + "=" * 60)
    print("ðŸ’¡ RECOMENDACIONES:")
    print("=" * 60)

    # Analyze bottlenecks
    if timings.get('db_get_all_draws', 0) > 100:
        print("  1. ðŸ“Š get_all_draws() es lento â†’ Considerar LIMIT o cache")
        print("     - Usar cache en memoria con TTL de 5 minutos")
        print("     - O calcular solo sobre Ãºltimos 200 draws")

    if timings.get('temporal_weights', 0) > 100:
        print("  2. ðŸ§® CÃ¡lculo temporal es pesado â†’ Pre-calcular en pipeline")
        print("     - Guardar hot/cold en tabla analytics_cache")
        print("     - Actualizar solo despuÃ©s de cada sorteo")

    if timings.get('momentum_analysis', 0) > 100:
        print("  3. ðŸ“ˆ AnÃ¡lisis de momentum costoso â†’ Simplificar algoritmo")

    if timings.get('gap_analysis', 0) > 100:
        print("  4. ðŸ“‰ Gap analysis costoso â†’ Optimizar numpy ops")

    if total_elapsed > 200:
        print("\n  ðŸš€ SOLUCIÃ“N RECOMENDADA: Implementar cache con TTL")
        print("     - Los datos solo cambian 3 veces por semana (Lun/Mie/Sab)")
        print("     - Cache de 5-10 minutos elimina 99% del trabajo")

    print()


def test_cache_performance():
    """Test the cache implementation performance"""

    print("\n" + "=" * 60)
    print("ðŸ§ª TEST DE RENDIMIENTO CON CACHE")
    print("=" * 60 + "\n")

    from src.v2.analytics_api import (
        get_analytics_overview,
        invalidate_analytics_cache,
        _is_cache_valid
    )
    import asyncio

    # Invalidate cache first
    print("1ï¸âƒ£  Invalidando cache...")
    invalidate_analytics_cache()
    print(f"   Cache vÃ¡lido: {_is_cache_valid()}")

    # First call - should calculate
    print("\n2ï¸âƒ£  Primera llamada (cÃ¡lculo completo)...")
    start = time.perf_counter()
    result1 = asyncio.run(get_analytics_overview())
    time1 = (time.perf_counter() - start) * 1000
    print(f"   â±ï¸  Tiempo: {time1:.2f}ms")
    print(f"   ðŸ“Š from_cache: {result1.from_cache}")
    print(f"   ðŸ“Š calculation_time_ms: {result1.calculation_time_ms}")
    print(f"   ðŸ“Š Hot numbers: {result1.hot_cold.hot_numbers[:5]}")

    # Second call - should use cache
    print("\n3ï¸âƒ£  Segunda llamada (desde cache)...")
    start = time.perf_counter()
    result2 = asyncio.run(get_analytics_overview())
    time2 = (time.perf_counter() - start) * 1000
    print(f"   â±ï¸  Tiempo: {time2:.2f}ms")
    print(f"   ðŸ“Š from_cache: {result2.from_cache}")
    print(f"   ðŸ“Š cache_age_seconds: {result2.cache_age_seconds}")

    # Third call - also from cache
    print("\n4ï¸âƒ£  Tercera llamada (desde cache)...")
    start = time.perf_counter()
    result3 = asyncio.run(get_analytics_overview())
    time3 = (time.perf_counter() - start) * 1000
    print(f"   â±ï¸  Tiempo: {time3:.2f}ms")
    print(f"   ðŸ“Š from_cache: {result3.from_cache}")
    print(f"   ðŸ“Š cache_age_seconds: {result3.cache_age_seconds}")

    # Summary
    print("\n" + "=" * 60)
    print("ðŸ“Š RESUMEN DE RENDIMIENTO CON CACHE")
    print("=" * 60)
    print(f"\n  Primera llamada (sin cache): {time1:,.2f}ms")
    print(f"  Segunda llamada (con cache): {time2:,.2f}ms")
    print(f"  Tercera llamada (con cache): {time3:,.2f}ms")

    speedup = time1 / time2 if time2 > 0 else 0
    print(f"\n  ðŸš€ MEJORA DE RENDIMIENTO: {speedup:,.0f}x mÃ¡s rÃ¡pido")
    print(f"  ðŸ’¾ Ahorro por request cacheado: {time1 - time2:,.2f}ms")

    if time2 < 10:
        print("\n  âœ… CACHE FUNCIONANDO CORRECTAMENTE")
        print("     Requests cacheados responden en <10ms")
    else:
        print("\n  âš ï¸  Cache puede necesitar ajustes")

    print()


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--cache":
        test_cache_performance()
    else:
        diagnose_analytics_endpoint()
        print("\n" + "-" * 60)
        print("ðŸ’¡ Para probar el cache ejecuta:")
        print("   python scripts/diagnose_hot_cold_endpoint.py --cache")
        print("-" * 60 + "\n")
